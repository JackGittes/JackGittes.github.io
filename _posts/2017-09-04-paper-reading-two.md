---
title: 网络压缩论文：On Compressing Deep Models by Low Rank and Sparse Decomposition
category: 网络压缩
author: 赵明心
excerpt: |
  这篇文章要总结的是CVPR 2017的一篇论文，是剪枝稀疏化、低秩分解、参数量化这三大神经网络压缩流派里面的低秩分解派。有趣的是，通讯作者IEEE Fellow 陶大程2000年前后在中科大指导学生写过一篇短文《DNA序列分类的数学模型》，前几年我在做数学建模比赛的时候曾经读过。没想到现在看ICLR的论文又遇到了陶老师指导的文章，真是有意思。
use_math: true
#feature_text: |
  ## The Pot Still
#  The modern pot still is a descendant of the alembic, an earlier distillation device
#feature_image: "https://unsplash.it/1200/400?image=1048"
#image: "https://unsplash.it/1200/400?image=1048"
---
### 文章简介
　　这篇CVPR论文利用了低秩分解和稀疏化的结合，但是这里的稀疏化并不是剪枝实现的稀疏化，而是借助于一个优化问题实现的。

　　主要分析一下文章思路。本质上，文章是在寻找一个对权重矩阵$W$的逼近矩阵$W'$，使这个$W'$能有比较好的数值特性(例如：元素的数值比较集中、矩阵比较稀疏等等)，来实现神经网络的压缩和加速。

　　他们的想法是将权重矩阵分解成低秩成分和稀疏成分，也即：

$$W\approx L+S$$

其中$L$是低秩成分，矩阵的秩不超过一个值$r$，而$S$是一个稀疏矩阵，矩阵的非零元很少。一个个人的理解是，这个想法就有点像把一个信号拆分成高频成分和低频成分，$L$就像信号的低频成分一样保留了基本的信息，而$S$对应着高频成分，记录的是细节信息。

　　有了这个思路以后，下面的事情就是把这个问题数学化，寻找恰当的$L$和$S$的过程就是要去满足下面这个式子(矩阵的数值逼近)：

$$mininize\ \ ||W-L-S||$$

文中实际使用的是$L2$范数，也即：


$$mininize\ \ \frac{1}{2}||W-L-S||^2$$

紧接着，文章把低秩成分$L$表达成了$L=UV$，$U\in R^{m\times r}$，$V\in R^{r\times k}$，这样做的好处是有点类似于SVD，可以减少参数的数目。

　　但这里除了数值逼近还需要考虑到一个问题，那就是逼近误差会在神经网络前馈过程中逐层累加。所以，为了消除神经网络层数过深而造成的层与层逼近误差累加过大，也为了更方便地将逼近问题转化成一个优化问题，文章假定某一层的输入向量$X=[x_1,x_2...x_n]$，优化前的输出向量为$Y=[y_1,y_2...y_n]$，要减少每层的输出误差，即要求：

$$
min\ \  \frac{1}{2n}||Y-(L+S)X||^2_F \\

s.t.\ \  \frac{1}{2}||W-L-S||^2_F \leq \gamma \\

rank(L)\leq r \\

card(S)\leq c \\
$$

根据前面的一系列数学表述和问题转化，对权重矩阵$W$的逼近问题一下回归到了这个组擅长的领域，他们把上述问题转换成了一个等价的目标函数：

$$
min\ \  \frac{1}{2n}||Y-(L+S)X||^2_F + \frac{\lambda}{2}||W-L-S||^2_F
$$

为求解这个优化问题，文章利用了他们组之前发的一篇文章里提出的一种基于QR分解的迭代算法，对$L$和$S$分别进行迭代求解。具体的迭代算法，此处先占个坑，有时间了再补充上。

---