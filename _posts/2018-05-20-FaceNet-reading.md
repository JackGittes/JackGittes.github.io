---
title: |
      读论文：FaceNet
category: 神经网络
author: 赵明心
excerpt: |
 这是谷歌在2015年发表于CVPR的论文，文章提出了一种使用神经网络进行度量学习的方法，并将构造的神经网络用于人脸识别，在LFW数据集上达到了当时的state-of-the-art水平。文章全名《FaceNet: A Unified Embedding for Face Recognition and Clustering》
use_math: true
#feature_text: |
  ## The Pot Still
#  The modern pot still is a descendant of the alembic, an earlier distillation device
#feature_image: "https://unsplash.it/1200/400?image=1048"
#image: "https://unsplash.it/1200/400?image=1048"
---

### 一、简介
度量学习是为了直接从数据样本中学得一种相似度的度量方式(也即一种度量空间，在这个空间中的距离直接与样本的相似性有关)。谷歌提出的FaceNet通过训练一个深度神经网络，使得神经网络的输出(一个n维的向量)直接对应于输入样本在度量空间中的表示。FaceNet可以直接学习到一种从人脸图片到一个紧欧氏空间的映射关系。

和传统神经网络使用的交叉熵损失不同，FaceNet使用了三元组损失(Triplet Loss)，这也是FaceNet能够学得一种度量的重要原因。使用了三元组损失之后，FaceNet不再需要像之前的自编码器等神经网络一样，必须要通过一个瓶颈层才能获得样本的低维嵌入表示，而是直接可以输出低维表示。

文章的主要创新点即在于：
- 使用了三元组损失(Triplet Loss)而不是传统的交叉熵损失来构造网络的损失函数
- 网络可以直接学到从样本空间到一个紧欧氏空间的映射关系
- 用这个方法可以实现一种架构完成多种任务

### 二、背景与引言
这个网络可以用一个架构实现三种任务：人脸验证(验证两张脸是否是同一个人)，人脸识别(识别人脸属于数据库中的哪个人)，聚类(将相似的人脸归为一类)。其核心在于他们使用深度神经网络来直接学习一个欧氏距离而非优化交叉熵损失，一旦训练完成，每个人脸样本对应的输出向量之间的距离就直接和人脸的相似度有关。

### 三、实现细节

#### 3.1 网络结构

下图是本文提出的系统结构，Deep Architecture可以是任意的神经网络。

<center>
<img src="http://wx2.sinaimg.cn/large/41f56ddcgy1frebjunlw0j21cp0cujtp.jpg" width="700px">
</center>

我觉得作者在中间加一个L2归一化是为了加快收敛速度，再者可以让输出向量直接被映射到一个紧空间(n维有限闭区间)里。

#### 3.2 三元组损失
计算一个三元组损失需要三个样本，其中一个样本被称为“Anchor”，另一个样本与“Anchor”来自同一个人脸，被称为“Positive(正样本)”，最后一个样本来自其他人的人脸，为“Negative(负样本)”。

<center>
<img src="http://wx3.sinaimg.cn/large/41f56ddcgy1frebjv1834j21160auab9.jpg" width="500px">
</center>

三元组损失被定义为：

<center>
<img src="http://wx1.sinaimg.cn/large/41f56ddcly1frebuivh4kj20qh03jaa3.jpg" width="400px">
</center>

最小化三元组损失，在直观上就是“最小化同类样本之间的距离，最大化和异类样本的距离”。这也使得网络输出的向量能够直接用来度量样本间的相似性。这里面的$\alpha$是一个超参数，不能通过训练得到，需要人为设定。

#### 3.3 三元组的构造方法
三元组构造的核心是选择“难分”的正样本和负样本，所谓“难分”是指到Anchor距离很大的正样本或者到Anchor距离很小的负样本。这些样本的三元组损失很大，也容易产生错误的分类结果，训练时的关键就是优化“难分”样本的向量表示。如果在构造训练数据的时候，尽可能多的选择“难分”样本来构成“Anchor、Positive、Negative”三元组会加快收敛速度(因为易分类样本的三元组损失本来就小，不会对模型参数更新产生大的影响)。

文章提出了离线和在线两种构造三元组的方法：

- 离线方法：训练网络，每N步中止一次，利用当前网络给出的距离度量来选择难分的正负样本组成三元组
- 在线方法：在训练过程当中，直接在每个小batch里选择最难分的正负样本构成三元组

正确选择三元组是算法收敛最重要因素。可以把离线和在线方法进行混合使用，作者认为这样做有可能加快算法收敛速度，但是没有得到实验验证。

#### 3.4 训练方法和过程
作者使用了传统的随机梯度下降(SGD)对网络进行训练，优化算法是反向传播，优化器用的是AdaGrad。模型在一个CPU集群上训练了1000~5000个小时。

### 四、实验结果
略

### 五、总结与结论

FaceNet使用Triple Loss替代Cross Entropy Loss，好处是可以直接学得一个欧氏空间的低维嵌入表示，但是Triple Loss并非本文首创，Google很巧妙地把它跟神经网络的强大表示特性结合才使得Triple Loss的效果发挥出来。这里仍然存在的一个问题是，Triple Loss的训练结果看似很好，但却很难收敛，尤其是非常依赖于困难样本的重新筛选。