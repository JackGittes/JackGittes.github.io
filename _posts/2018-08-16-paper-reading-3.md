---
title: |
      读论文：AlexNet
category: 神经网络
author: 赵明心
excerpt: |
 本文是2012年发表的在ImageNet上进行分类的神经网络。
use_math: true
#feature_text: |
  ## The Pot Still
#  The modern pot still is a descendant of the alembic, an earlier distillation device
#feature_image: "https://unsplash.it/1200/400?image=1048"
#image: "https://unsplash.it/1200/400?image=1048"
---

### 一、概览

AlexNet网络结构：
- 八层网络
- 五层卷积、三层全连接

### 二、网络性能提升的因素
文章总结了能使AlexNet精度、训练速度以及泛化能力得到提升的主要因素，以下按重要性列举：

#### 2.1 ReLU 非线性激活
文章发现ReLU激活函数比普通的饱和型激活函数(Tanh，Sigmoid)收敛速度更快，在CIFAR-10数据集上测试的时候，达到同样的精度和收敛程度，ReLU要快6倍左右。

这里作者使用了一个技巧，在训练大型神经网络的时候，可以预先在小一些的数据集上进行测试，然后再转移到大的数据集上，这样可以更快速方便地寻找合适的网络配置方案。

#### 2.2 多GPU训练
文章使用了两块GPU进行训练，这里不再描述，现在使用GPU训练已经是训练神经网络所必须的了。

#### 2.3 LRN(局部响应归一化)


#### 2.4 重叠池化
当池化步长小于池化窗口大小的时候是重叠池化，文章发现重叠池化可以略微降低网络的错误率(Top-1大概降低0.4%，Top-5大概0.3%)，他们使用的池化窗口大小为3，步长为2。

#### 2.5 防止过拟合 —— 数据增强
使用运行在CPU上的Python代码来对图片数据进行变换处理，这个过程和GPU上进行的训练过程是同步的，GPU在使用上一批次处理好的图片进行训练的同时，CPU在对新一批的图片数据进行增强处理，所以整个数据增强不会造成太大的运算负担。图片变换操作主要包含了两种：
##### 2.5.1 图片翻转
文章使用了224×224大小的图像块来替代整个图像，每张图像会被随机裁切成这些大小的图片块，同时也包括水平镜像翻转的图像块，通过增加这些数据可以有效防止过拟合。

##### 2.5.2 PCA处理

#### 2.6 防止过拟合 —— Dropout

### 三、其他细节

### 四、总结