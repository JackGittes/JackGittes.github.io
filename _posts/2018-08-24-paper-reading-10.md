---
title: |
    神经网络「压缩、量化、加速」发展综述
category: 网络压缩
author: 赵明心
excerpt: |
 「压缩、量化、加速」是伴随神经网络兴起以来的一个新研究领域，大部分深度网络的参数和运算量都远远超过经典模型，这导致深度神经网络虽然精度高，但却无法被部署在移动和嵌入式设备上。对神经网络进行压缩和加速成为了一个研究热点。
use_math: true
#feature_text: |
  ## The Pot Still
#  The modern pot still is a descendant of the alembic, an earlier distillation device
#feature_image: "https://unsplash.it/1200/400?image=1048"
#image: "https://unsplash.it/1200/400?image=1048"
---

### 一、概要

近些年来深度神经网络已经取得了显著的进展，在计算机视觉、自然语言处理、语音识别等领域的很多任务上，DNN的表现已经超越了人类。但是，DNN对CPU和GPU资源的消耗使得它无法被部署在嵌入式系统和移动设备上，例如无人机、智能手机、智能眼镜。这些设备的计算资源不多，而且电池容量也相当有限。

为了解决此类问题，学术界和工业界提出了很多网络压缩和加速方案。一般而言，CNN的计算复杂度主要体现在卷积层，而它的存储消耗主要来自于全连接层的大量参数。因此，多数加速方案关注减少卷积复杂度，而压缩方案则关注于压缩全连接层参数。

现今主流的压缩和加速方案如下：
- 网络剪枝 
- 低秩分解
- 网络量化
- 师生网络
- 紧凑型网络设计

本文内容主要来自自动化所的程健老师在2018年发表的综述文章《Recent Advances in Efficient Computation of
Deep Convolutional Neural Networks》。

### 二、主流的压缩与加速方案

#### 2.1 网络剪枝
##### 2.1.1 精细剪枝

##### 2.1.2 向量级和Kernel级剪枝

##### 2.1.3 组级剪枝

##### 2.1.4 滤波器级剪枝

#### 2.2 低秩分解

##### 2.2.1 两元分解

##### 2.2.2 三元分解

##### 2.2.3 四元分解


#### 2.3 网络量化

##### 2.3.1 标量与向量量化

##### 2.3.2 权重的定点数量化

##### 2.3.3 激活值定点数量化


#### 2.4 师生网络


#### 2.5 紧凑型网络

压缩和加速的一个主要目标就是为了优化深度网络的执行和存储框架，设计一个更加有效、消耗更低的网络也是一种方法。

在《Network in network》中，作者提出了一种方案，可以在网络架构中大量使用1×1大小的卷积核来增加网络容量同时又使计算复杂度足够小。同时，为了减少CNN的存储需求，他们舍弃了全连接层，取而代之的是一个全局的均值池化。这些技巧也被后续很多像GoogleNet和ResNet的SOTA的网络使用。

分支(多组卷积)是另一种常被用来降低网络复杂度的方案。最早在GoogleNet中出现，后来提出的SqueezeNet在AlexNet上实现了50倍的压缩效果。借助分支策略，ResNeXt在同样计算开销的情况下达到了比ResNet更高的精度。

MobileNet规模是VGG-16的$\frac{1}{32}$，但运行速度却是VGG-16的27倍，精度与VGG-16相当。MobileNet使用了在深度上的卷积操作并且有大量1×1的卷积核，绝大多数运算也基本集中在1×1卷积运算上。

进一步降低1×1卷积复杂度的方法是使用多组卷积。ShuffleNet引入通道

### 三、硬件加速设计

#### 3.1 通用架构


#### 3.2 处理单元


#### 3.3 高吞吐量优化


#### 3.4 低功耗优化

#### 3.5 设计自动化

#### 3.6 正在涌现的技术

### 四、未来发展趋势


### 五、结论
